{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSg7sPhQIx9G",
        "outputId": "0c3b0323-42ef-41aa-fd5d-ef00af83dc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /opt/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.29.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: sklearn in /opt/anaconda3/lib/python3.11/site-packages (0.0.post7)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pytorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: pytorch\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
            "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
            "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-uwdzr73g/pytorch_735a2009ccac469d957fb4b97750b849/setup.py\", line 15, in <module>\n",
            "  \u001b[31m   \u001b[0m     raise Exception(message)\n",
            "  \u001b[31m   \u001b[0m Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchvision (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orchaudio (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/anaconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install pytorch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertModel, BertTokenizer,TrainingArguments, Trainer, BatchEncoding\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "%pip install transformers\n",
        "%pip install sklearn\n",
        "%pip install pytorch\n",
        "\n",
        "from transformers import BertModel, BertTokenizer,TrainingArguments, Trainer, BatchEncoding\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20qEwu_1TpQs"
      },
      "source": [
        "## Preparação do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwEQsOoULA4m"
      },
      "outputs": [],
      "source": [
        "class SequenceViabilityDataset:\n",
        "  def __init__(self, sequences, labels):\n",
        "    \n",
        "    self.sequences = sequences\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return self.sequences[i], self.labels[i]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sequences)\n",
        "\n",
        "  def get_idx_split(self, train_size=0.8):\n",
        "    all_ids = range(len(self.sequences))\n",
        "    train_ids, test_ids = train_test_split(all_ids, train_size = 0.8)\n",
        "    test_ids, val_ids = train_test_split(test_ids, train_size = 0.5)\n",
        "    return {\"train\": train_ids, \"test\": test_ids, \"valid\": val_ids}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S43LqZ8CT7m8"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"./datasets/protein_mutation_viability.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_data = []\n",
        "for index, row in df.iterrows():\n",
        "  spl = row.array[0].split(\";\")\n",
        "  s, l = spl[0], spl[-1]\n",
        "  l = 0 if l == \"FALSE\" else 1\n",
        "  s = \" \".join(list(s.upper()))\n",
        "  s = re.sub(r\"[UZOB]\", \"X\", s)\n",
        "  all_data.append((s, l))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instanciação do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hgPwSMeNbCm",
        "outputId": "746d458f-7081-4232-980b-83c8274a18cc"
      },
      "outputs": [],
      "source": [
        "dataset = SequenceViabilityDataset([x[0] for x in all_data], [x[1] for x in all_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train test validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_idx = dataset.get_idx_split()\n",
        "train_data = [dataset[i] for i in split_idx[\"train\"]]\n",
        "test_data = [dataset[i] for i in split_idx[\"test\"]]\n",
        "valid_data = [dataset[i] for i in split_idx[\"valid\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Criação de batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('D E D E I A C T N P V A T E Q Y G S V A T N L E R G N E K D', 'D E E E I R T T N P V A T E Q Y G S V H T N L Q R F N R', 'C E E E I A C T N P V A T E S Y G V C T A C A A N N E S E T E H G N D T Q', 'D E E E I R T T N P V A T E Q Y G S V S T A L D Q R S G N E R N', 'D E E E I R T T N P V A T E Q Y G S V S T N L Q C R F G N R', 'D E E E I R T T N P V A T E Q Y G S V S T L N T G G Q G G N T', 'Q V P E G I I L Q T T H C V I V H A T K Q Y G I V E T Y N L Q R W A N R', 'F E T G E I R T T S N P D A A T E Q P T G N K Q S S M N L A C C G Y N Y R P', 'M E D E V C T T N G P V A T E Q Y W G S V V S I T M N L Q M H E N N R L', 'D E E E I S T T N P V A Y E Q Y G Q C V C S W G E H N A E N F N S E T Q A') tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1])\n",
            "('D E D E I A T T N P V A T E Q W G C V A G T G P P N S M N P Q D H Q', 'D E E E I R T T N P V A T E Q Y G S V E E V A T G L Q G R G V V E L F', 'D E G E I R T T N F V A T E Q Y G S V D E E S N L E Q R T G N M', 'S M E L I M T C N P T A S D C F A F L H A N Y I Q G P F', 'Q E E E I T C T N P V A T E N Y G V T G E T G E L Q V V E G E N G', 'D P E P I R T F N P D A T I Q Q Y G Q W W M N M H M C S A D T N G Q G L V F F', 'A E E E I R T T N P V A T E Q Y G S V S V T N Q Q G N L G I N T G', 'V E V E M R M V Q P P A T V K T I N S Y S F V S C T M N N Q Q E L G G E A', 'D E E E I R T T N P V A T E Q Y G S V N S T P N L Q F N F G N Q N T', 'D E E E I R T T N P V A T E Q F Y G C V V S S N W L Q N T Q E R') tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0])\n",
            "('D E E E I R T T N P V A T E Q Y G S V S P T D N L T T R N S D H G', 'D E A T E V E I A T N P V A Y E Q W G T V C S T D N G D Q H Q G H N Q M A', 'D E M E I R A T N P V A T E Q Y G E V S T G N L G Q T G G N Q R', 'A E E C I S T T N P V A Y E Q Y G Q C V C S N W G E D D N A E N F N S E T Q M', 'D E E E I R T T N P E A T E Q Y G S V E T N D Q R E N F', 'T E E E I R T T N P V A T Q H Y G Y S A S C T A L A L R H G E N Q R', 'D D E E I R T T N P V A Y E G Y G Q C V C S N W G E D H N A E N F N S D T Q L', 'D E A E I R T T N P V A T E Q Y G S V C F S V Y N Q Q E E F D G K Q E T', 'D E E E I R T T N P V A T E Q Y G S V S E P V L Q N L V N D L', 'D E E E I R T T N Q P V A T E Q Y G S V S T R L L Q R S G N R') tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0])\n",
            "('N E E I I I T T N P M A M E Q W Y G S S V S A S T N L T Q V G N I N', 'D E E E M R E M T N T V V A T I Q V G L T I M V T T E N L G S E G E T E G E', 'D E P E E I E T T N P V A T E Q Y G D T Y C A G C L E M V A Q Q M A V A V H D', 'E T A E I R T T N P V A Y E Q W G S V C Q T D N H D G Q R T G Q D D R', 'D E E E I R T T N P V A T E Q Y G S V S G T N N D G Q N N G N E', 'D E E E I R T T N P V A T E Q Y G S V Y P T M L P E T R P V S V E Q', 'D E D E I A T T N P V A Y A P W G T F A C A A T A G E L P D S L A S N N R N', 'D E E E I R T T N P V S T E Q Y G S V S C D L Q D Y G N D R', 'M E E E I R T T N P V A T E Q Y G D V V T G Q T G L Q D E G G I G G T', 'D E E D V I R T E Y S F K S Q Y Y W G I S H Y S A K V Q W Q K D W G E D A T') tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0])\n",
            "('D E E E I R T T N P V A T E Q Y G E V S T N L Q H G N R', 'V E E E V C T T N V P V A T E Q Y Y G Q D V V S I T M N L Q M H E N N R Q', 'D Q N I I R A R N P S G T E G Y G V L V C C T F E C Y Q H V A M E R', 'D E E E I R T T N P V S T E H F G S V S D N G Q R G N V', 'D E E E I R T T N V V D D E T V D Y G N F I N Y L T F V S E K V A S V G N P P E I', 'D E E E I R T T N P V A T E Q Y G S V S T N L Q R G N R L', 'D E E E I H R T T N P V Q A T E Q Y G S V S T N L Q R G N R', 'D E E E I S T T N P V A T E Q Y G C V C S T E L N Q N F G S N N Q G', 'A E D E I A T T N P V A T E Q Y G S V S T N L Q R D G N E D Q', 'D E E E I R T T N P V A T E Q Y G C V C S N T E L Q H F N N R Q') tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "c = 0\n",
        "for i, l in train_loader:\n",
        "  if c < 5:\n",
        "    print(i, l)\n",
        "  c += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definição do classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMBF6ZwBBSam"
      },
      "outputs": [],
      "source": [
        "class ProtBERTClassifier(torch.nn.Module):\n",
        "    def __init__(self, model, num_classes):\n",
        "        super(ProtBERTClassifier, self).__init__()\n",
        "        self.model = model\n",
        "        self.classify = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "\n",
        "        bert_embeddings = self.model(**BatchEncoding(data=sequences)).last_hidden_state.to(device)\n",
        "\n",
        "        return self.classify(bert_embeddings).squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instanciação do classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AcNi8JKwNDVu",
        "outputId": "6abd0c2c-168d-4086-dfec-e912a65fcea1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ProtBERTClassifier(\n",
              "  (model): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(40000, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-29): 30 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classify): Linear(in_features=1024, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
        "\n",
        "bert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
        "bert_model.to(device)\n",
        "model = ProtBERTClassifier(bert_model, 1)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definição do training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_kwrzuAJfdx"
      },
      "outputs": [],
      "source": [
        "def test_model(model, tokenizer, test_dataloader):\n",
        "    num_correct = 0\n",
        "    num_tests = 0\n",
        "    with torch.no_grad():\n",
        "        for batched_sequences, labels in test_dataloader:\n",
        "            #print(batched_sequences, labels)\n",
        "            tokens = tokenizer(batched_sequences, return_tensors='pt', padding=True).to(device)\n",
        "            #print(2)\n",
        "            pred = model(tokens)\n",
        "            num_correct += (pred.argmax(1) == torch.Tensor(labels).to(device)).sum().item()\n",
        "            num_tests += len(labels)\n",
        "\n",
        "    accuracy = num_correct / num_tests\n",
        "    print(\"acc =\", accuracy)\n",
        "    return accuracy\n",
        "\n",
        "def train_model(model, tokenizer, train_dataloader, valid_dataloader, epochs=10, lr=0.01):\n",
        "  \n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "  patience = 3\n",
        "  delta = 0.5\n",
        "  c = 0\n",
        "  min_so_far = np.inf\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      print(\"Epoch\", epoch+1)\n",
        "\n",
        "      # Training loss\n",
        "      for batched_sequences, labels in train_dataloader:\n",
        "\n",
        "\n",
        "          tokens = tokenizer(batched_sequences, return_tensors='pt', padding=True).to(device)\n",
        "\n",
        "          pred = model(tokens)\n",
        "\n",
        "          training_loss = F.cross_entropy(pred, torch.Tensor(labels).to(device))\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          training_loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "      # Early stopping\n",
        "      # Comparar validation loss com training loss\n",
        "      \n",
        "      test_model(model, tokenizer, test_loader)\n",
        "\n",
        "      # Sem afetar o modelo\n",
        "      total_validation_loss = 0\n",
        "      with torch.no_grad():\n",
        "        for batched_sequences, labels in valid_dataloader:\n",
        "\n",
        "            tokens = tokenizer(batched_sequences, return_tensors='pt', padding=True).to(device)\n",
        "\n",
        "            pred = model(tokens)\n",
        "\n",
        "            valid_loss = F.cross_entropy(pred, torch.Tensor(labels).to(device))\n",
        "            \n",
        "            total_validation_loss += valid_loss.item()\n",
        "            \n",
        "\n",
        "      # Se a validation loss for a menor até agora, atualizar e dar reset ao counter\n",
        "      if total_validation_loss < min_so_far:\n",
        "        min_so_far = valid_loss.item()\n",
        "        c = 0\n",
        "      # Se a validation loss for maior que a menor até agora + delta, incrementar counter\n",
        "      elif total_validation_loss > min_so_far + delta:\n",
        "        c += 1\n",
        "\n",
        "        # Se a validation_loss aumentou vezes suficientes de seguida, parar\n",
        "        if c >= patience:\n",
        "          break\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "train_model() got multiple values for argument 'epochs'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model \u001b[39m=\u001b[39m train_model(model, tokenizer, train_loader, test_loader, valid_loader, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: train_model() got multiple values for argument 'epochs'"
          ]
        }
      ],
      "source": [
        "\n",
        "trained_model = train_model(model, tokenizer, train_loader, valid_loader, epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
